---
title: "rrq"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{rrq}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Getting started

This document assumes that that you have a Redis server running. If not, see the bottom of the document for options you will have for installing this on your own system. You can test if your Redis server is behaving as expected by running

```{r}
redux::hiredis()$PING()
```

If you get an error like "Connection refused" then check your installation.

The package is designed to be easy to get started with, and has features that you might like to use later. If you run the "Hello world" section you probably have 90% of what you need - the result of the document will show features that will help you bend that around your specific needs.

## Hello world

Without any great explanation, here is the basic approach to using rrq to run a task on another R process, asynchronously . First, we create a "controller" object which you can use for queuing tasks.

```{r}
id <- paste0("rrq:", ids::random_id(bytes = 4))
obj <- rrq::rrq_controller(id)
```

This controller uses an "identifier" (here, `r id`) which can be anything you want but acts like a folder within the Redis server, distinguishing your queue from any others hosted on the same server.

Submit a task to the queue with the `$enqueue()` method, returning a key for that task

```{r}
t <- obj$enqueue(1 + 1)
t
```

We'll also need some worker processes to carry out our tasks. Here, we'll spawn two for now (see the section below on alternatives to this)

```{r}
rrq::worker_spawn(obj, 2)
```

Retrive the result, polling if needed:

```{r}
obj$task_wait(t, progress = FALSE)
```

Things to note here:

* Every task is given a unique identifier, which can be used to query the task later (much more on this below)
* The process with the controller is not blocked during this operation and could carry out any other calculation it wanted (it could even be closed and the controller recreated later)
* We submitted the tasks before we had workers ready; we could have done this in the other order, and workers can be added and removed at will regardless of the state of the queue (see below)

## Running tasks

Running tasks is a little different to many R-parallel backends, because we do not directly allocate tasks to our workers, but simply place it on first-in-first-out task queue.  A pool of workers will then poll for work from this queue.  See `vignette("design")` for more on this.

Consider enqueing this expression

```{r}
t <- obj$enqueue({Sys.sleep(2); runif(1)})
```

This has created a task that will sleep for 2 seconds then return a random number

```{r, include = FALSE}
Sys.sleep(0.5)
```

Initially the task has status `RUNNING` (it will be `PENDING` *very* briefly):

```{r}
obj$task_status(t)
```

Then after a couple of seconds it will complete (we pad this out here so that it will complete even on slow systems)

```{r}
Sys.sleep(3)
obj$task_result(t)
```

The basic task lifecycle is this:

* when created/submitted a task becomes `PENDING`
* when it is picked up by a worker it becomes `RUNNING`
* when it completes it will become `COMPLETE` or `ERROR`

In addition there are rarer ways a task can end (`CANCELLED`, `DIED`, `TIMEOUT`) or fail to start (`DEFERRED` or `IMPOSSIBLE`)

Above, we slept for a few seconds in order for the task to become finished. However, this is an extremely common operation, so `rrq` provides a `$task_wait` method which will wait until a task finishes, then returns the result.


```{r}
t <- obj$enqueue({Sys.sleep(2); runif(1)})
obj$task_wait(t)
```

The polling interval here is 1s by default, but if the task completes within that period it will still be returned as soon as it is complete (the interval is just the time between progress bar updates and the period where an interrupt would be caught to cancel the wait).

Once a task is complete, `$task_wait` and `$task_result` are equivalent

```{r}
obj$task_wait(t)
obj$task_result(t)
```

## Configuring the worker environment

It is rare that we want our workers to run in completely empty R environments (no extra loaded packages, no custom functions available). Quite often you will want to run something to configure the workers before they accept jobs.

In order to do this, first define a function that will accept one argument which will be the environment that the worker will use, and then set that environment up. So, for example, suppose we want to source a file "myfuns.R" which contains some code

```{r results = "asis"}
writeLines(c("```r", readLines("myfuns.R"), "```"))
```

We might write:

```{r}
create <- function(env) {
  sys.source("myfuns.R", env)
}
```

This approach would also allow you do do somethng like read an rds or rdata file containing a large object that you want every worker to have a copy of.

The next step is to register this function for your queue:

```{r}
obj$envir(create)
```

By default, this will notify all running workers to update their environment. Note that if your function errors in any way, your workers will exit!

```{r}
obj$worker_log_tail(n = 4)
```

Now our workers have picked up our functions we can start using them:

```{r}
t <- obj$enqueue(slowdouble(1))
obj$task_wait(t)
```

## Scheduling options

The `rrq` package does not aspire to be a fully fledged scheduler, but sometimes a little more control than first-in-first-out is required. There are a few options available that allow the user to control how tasks are run when needed. These involve:

* blocking tasks so that they run only after other tasks are completed (e.g., a set up task followed by a series of computational tasks, followed by an aggregation task)
* multiple queues with different priorities, or that different workers subscribe to (e.g., fast/slow high-priority/low-priority tasks, or tasks that require a limited resource)
* put tasks in the front of the queue, allowing a last-in-first-out queue

### Tasks that depend on other tasks

We support a simple system for allowing tasks to depend on other tasks. An example of this might be where you need to download a file, then run a series of analyses on it. Or where you want to run an analysis over a set of parameters, and then aggregate once they're all done. How the output of one task feeds into the others is up to you, but practically this will require one of the following options:

* write your results to disk and read them back
* store results in a database (e.g., Redis!) at the end of one task, read them in the next
* connect to the queue from your task itself

When queueing a task, you can provide a vector of task identifiers as the `depends_on` argument. These identifiers must all be known to `rrq` and the job will not be started until all these prerequisites are completed. The task lifecycle will look like different to the above; rather than starting as `PENDING` the task begins as `DEFERRED`.

Once all prerequisites are complete, a task becomes possible and it moves from `DEFERRED` to `PENDING`. It will be placed at the *front* of the queue.

If a prerequisite task fails for any reason (an error, is cancelled, or its worker dies) then the task will become `IMPOSSIBLE`.

### Multiple queues

Sometimes it is useful to have different workers listen on different queues. For example, you may have workers on different machines with different capabilities (e.g., a machine with a GPU or high memory). You may have jobs that are expected to take quite a long time but want some workers to monitor a fast queue with short lived jobs.

### Jump the queue

Almost always you will want to use a first-in-first-out queueing system, which is the default that rrq supports. However when queeing you can specify `at_front = TRUE` and jump to the front of the queue.

## Running tasks in separate processes

Running a task in a separate process offers some additional features at a cost of a little more overhead per task.

The cost is that we have to launch an additional process for every task run. We use [`callr`](https://callr.r-lib.org/) for this to smooth over a number of rough edges, but this does impose a minimum overhead of about 0.1s per task, plus the cost of loading any packages that your task might need (if you use packages that make heavy use of things like S4 classes this can easily extend to a few seconds).

The additional features that it provides are:

* Per-task isolation: because every task runs in a separate process it works in a fresh environment and is isolated from all other tasks
* Cancellable tasks: you can stop a running task and the worker will gracefully pick up additional work
* Per-task timeout: you can specify the maximum running time of any task and if the task exceeds it, it will be killed

The sorts of tasks that benefit from this sort of approach are typically long-running (expected running times in the 10s of seconds or more) so that the overhead is low, but also the features of cancellation and timeouts become more useful. We have also seen this used usefully where the task may leak memory, or cache results agressively - over time this would cause the worker process to consume more memory until the worker process was killed by the operating system.

## Orchestrating workers

This vignette uses the very basic `worker_spawn()` method to create workers on your local machine. This is intended primarily for development only, though it may be useful in some situations.

### Use (and limitations) of spawn

### Use a scheduler

If you are using rrq with an HPC system, then you will want to schedule workers onto the system. To do this...

### Use docker

We provide a docker image that you can use

## Configuring workers

Timeouts, etc, redis connections

## Fault tolerance

Explanation of the strategy, show the configuration options

There is of course no strategy for 

## Getting a Redis server

There are several options to get started with Redis, the best one will likely depend on your platform and needs.

### Use docker

(Linux, macOS with [docker desktop](https://docs.docker.com/docker-for-mac/install/), Windows with [docker desktop](https://docs.docker.com/docker-for-windows/install/))

This is how we develop rrq because it's easy to destroy and recreate the redis instance. Start the docker redis container like:

```
docker run --name redis --rm -d -p 127.0.0.1:6379:6379 redis
```

This will listen on port 6379 which is the Redis default. You can stop the container (deleting all data) with `docker stop redis`

### Install Redis

On Linux this is fairly straightforward, either by [downloading and building the source code](https://redis.io/download) or by installing via `apt` or `snap`

On macOS the source will compile, or you can install a redis server via homebrew

On Windows you can [install redis via WSL](https://redislabs.com/blog/redis-on-windows-10/). There have also been various ports.

### Use a different server

If you have redis running on a different machine (this will be the case if you're using redis to distribute tasks over a number of different machines) you will need to tell `rrq` and `redux` where to find it. The simplest way is to set the environment variable `REDIS_HOST` to the name of the machine if it is running with default ports, or set `REDUX_URL` if you need more control. Alternatively, when connecting to the server above, you can manually construct your `redux::hiredis` object and pass in any configuration option you need; see the documentation for `redux::redis_config` for details.
